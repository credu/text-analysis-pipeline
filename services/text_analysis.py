# -*- coding: utf-8 -*-
"""pln_project_grupo_2_1p

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dMaqC_fLa9wHE2JWAq6N7bof61PZ20Qt

# Integrantes
- Jose Alejandro Alvarez Sanchez
- Justyn Gabriel Garcia Figueroa
- Jesus Jeampool Mendoza Navarro
- Daniel Moises Troya Riofrio

# Inicialización
"""

import re
import numpy as np
import string
from collections import defaultdict, Counter
import json
import matplotlib.pyplot as plt
from typing import List, Dict, Tuple, Optional

# Corpus proporcionado para entrenamiento
CORPUS = [
    "<s> el/DT señor/NN vino/VBD tarde/RB </s>",
    "<s> la/DT mujer/NN corre/VBP rápido/RB </s>",
    "<s> un/DT gato/NN salta/VBP alto/JJ </s>",
    "<s> los/DT perros/NNS ladran/VBP fuerte/RB </s>"
]

# Gramática en CNF
GRAMMAR_CNF = {
    'O': [('SN', 'SV', 1.0)],
    'SN': [('DT', 'N', 1.0)],
    'SV': [('V', 'SAdv', 0.5), ('V', 'SN', 0.5)],
    'SP': [('Prep', 'SN', 1.0)],
    'DT': [('el', 0.25), ('la', 0.25), ('un', 0.25), ('los', 0.25)],
    'N': [('señor', 0.2), ('mujer', 0.2), ('gato', 0.2), ('perros', 0.2), ('biblioteca', 0.2)],
    'V': [('vino', 0.25), ('corre', 0.25), ('salta', 0.25), ('ladran', 0.25)],
    'Prep': [('en', 0.5), ('de', 0.5)],
    'SAdv': [('RB', 0.5), ('JJ', 0.5)],
    'RB': [('tarde', 0.33), ('rápido', 0.33), ('fuerte', 0.33)],
    'JJ': [('alto', 1.0)]
}

"""# Clase Preprocessor (Jesus Mendoza)
Responsable de tokenización, normalización, eliminación de stopwords y lematización básica.
## Recursos
[Archivo de StopWords](https://raw.githubusercontent.com/stopwords-iso/stopwords-es/refs/heads/master/stopwords-es.txt)
"""

class Preprocessor:
    def __init__(self, *args):
        """Inicializa el preprocesador con una lista de stopwords (opcional)."""
        stopwords_file = args[0] if args else None
        self.stopwords = self.load_stopwords(stopwords_file) if stopwords_file else set([
            'el', 'la', 'los', 'un', 'de', 'en', 'y', 'a', 'que'  # Ejemplo básico
        ])
        self.lemmatization_rules = [
            (r'ando$', 'ar'),  # corriendo -> correr
            (r'iendo$', 'er'),  # comiendo -> comer
            (r's$', '')  # gatos -> gato
        ]

    def load_stopwords(self, *args):
        """Carga stopwords desde un archivo de texto (una por línea)."""
        filepath = args[0] if args else None
        with open(filepath, "r", encoding="UTF-8") as file:
            content = file.read().splitlines()
            return set(content)

    def tokenize(self, *args):
        """Divide el texto en tokens, eliminando puntuación básica."""
        text = args[0] if args else ""

        return re.findall(r"([().]|['\w]+)", text)

    def normalize(self, *args):
        """Convierte tokens a minúsculas y elimina caracteres no alfanuméricos."""
        tokens = args[0] if args else []

        return [word.lower() for word in tokens if word not in string.punctuation]

    def remove_stopwords(self, *args):
        """Elimina stopwords de la lista de tokens."""
        tokens = args[0] if args else []

        return [word for word in tokens if word not in self.stopwords]

    def lemmatize(self, *args):
        """Aplica reglas morfológicas simples para lematización."""
        tokens = args[0] if args else []

        lemmas = []
        for word in tokens:
            for rule in self.lemmatization_rules:
                regex, replacement = rule
                if re.search(regex, word):
                    lemma = re.sub(regex, replacement, word)
                    lemmas.append(lemma)
                    break

        return lemmas

    def process(self, *args):
        """Ejecuta el pipeline completo: tokenización, normalización, eliminación de stopwords, lematización."""
        text = args[0] if args else ""

        tokenized_text = self.tokenize(text)
        normalized_tokens = self.normalize(tokenized_text)
        tokens = self.remove_stopwords(normalized_tokens)
        lemmas = self.lemmatize(tokens);

        return tokens, lemmas  # Retorna (tokens, lemas)

# Tes del pipeline completo
# preprocessor = Preprocessor("stopwords-es.txt")
# preprocessor.process("Un elefante corriendo se balanceaba sobre la tela de una araña.")

"""# Clase MorphologicalAnalyzer

---


Implementa POS tagging con HMM y Viterbi, y soporte para lematización.
"""

class MorphologicalAnalyzer:
    def __init__(self, *args):
        """Inicializa el analizador morfológico con un corpus etiquetado."""
        corpus = args[0] if args else []
        self.tags = set(['DT', 'NN', 'NNS', 'VBD', 'VBP', 'RB', 'JJ', '<s>', '</s>'])
        self.transition_probs = defaultdict(lambda: defaultdict(float))
        self.emission_probs = defaultdict(lambda: defaultdict(float))
        self.train(corpus)

    def train(self, *args):
        """Calcula probabilidades de transición y emisión a partir del corpus."""
        corpus = args[0] if args else []
        # TODO: Implementar conteo de transiciones y emisiones
        pass

    def viterbi(self, *args):
        """Implementa el algoritmo de Viterbi para POS tagging."""
        tokens = args[0] if args else []
        # TODO: Implementar Viterbi para encontrar la secuencia de etiquetas más probable
        return []

    def handle_unknown_word(self, *args):
        """Asigna probabilidades bajas a palabras desconocidas basadas en heurísticas."""
        word = args[0] if args else ""
        # TODO: Implementar heurísticas (ej., palabras terminadas en -mente como RB)
        return {}

    def tag(self, *args):
        """Etiqueta una lista de tokens con sus categorías morfosintácticas."""
        tokens = args[0] if args else []
        # TODO: Usar Viterbi para etiquetado
        return []

"""# Clase SyntacticAnalyzer
Implementa análisis sintáctico con el algoritmo CKY.
"""

class SyntacticAnalyzer:
    def __init__(self, *args):
        """Inicializa el analizador sintáctico con una gramática en CNF."""
        grammar = args[0] if args else {}
        self.grammar = grammar
        self.non_terminals = set(grammar.keys())
        self.terminals = set()
        self.build_terminal_set()

    def build_terminal_set(self, *args):
        """Construye el conjunto de terminales a partir de la gramática."""
        # TODO: Extraer terminales de la gramática
        pass

    def cky_parse(self, *args):
        """Implementa el algoritmo CKY para construir el árbol sintáctico."""
        tokens = args[0] if args else []
        # TODO: Implementar CKY para generar el árbol
        return None

    def build_tree(self, *args):
        """Construye el árbol sintáctico a partir de los backpointers del CKY."""
        backpointers, start, end, symbol = args[:4] if len(args) >= 4 else ({}, 0, 0, '')
        # TODO: Implementar reconstrucción del árbol
        return {}

    def visualize_tree(self, *args):
        """Visualiza el árbol sintáctico usando matplotlib o texto."""
        tree = args[0] if args else {}
        # TODO: Implementar visualización (texto o gráfica)
        pass

"""# Pipeline Completo
Integra las clases anteriores en un pipeline unificado.
"""

class TextAnalysisPipeline:
    def __init__(self, *args, stopwords_path=None):
        """Inicializa el pipeline con los componentes necesarios."""
        corpus, grammar = args[:2] if len(args) >= 2 else ([], {})

        self.preprocessor = Preprocessor(stopwords_path)
        self.morph_analyzer = MorphologicalAnalyzer(corpus)
        self.synt_analyzer = SyntacticAnalyzer(grammar)

    def process(self, *args):
        """Procesa un texto a través del pipeline completo."""
        text = args[0] if args else ""
        # Preprocesamiento
        tokens, lemmas = self.preprocessor.process(text)

        # Análisis morfológico
        tagged_tokens = self.morph_analyzer.tag(tokens)

        # Análisis sintáctico
        parse_tree = self.synt_analyzer.cky_parse(tokens)

        return {
            'tokens': tokens,
            'lemmas': lemmas,
            'pos_tags': tagged_tokens,
            'parse_tree': parse_tree
        }

    def visualize_results(self, *args):
        """Visualiza los resultados del pipeline."""
        results = args[0] if args else {}
        print("Tokens:", results.get('tokens', []))
        print("Lemas:", results.get('lemmas', []))
        print("POS Tags:", results.get('pos_tags', []))
        print("Árbol Sintáctico:")
        self.synt_analyzer.visualize_tree(results.get('parse_tree', {}))

"""# Demostración
Prueba el pipeline con oraciones de ejemplo.
"""

# Inicializar el pipeline
pipeline = TextAnalysisPipeline(CORPUS, GRAMMAR_CNF)

# Oraciones de prueba
test_sentences = [
    "el gato salta alto",
    "la mujer corre rápido",
    "los perros ladran fuerte",
    "el señor vino tarde",
    "un gato en la biblioteca"
]

# Procesar y visualizar resultados
# for sentence in test_sentences:
#     print(f"\nProcesando: {sentence}")
#     results = pipeline.process(sentence)
#     pipeline.visualize_results(results)

"""# Script de Demostración
Script para procesar un archivo de texto y generar resultados.
"""

def process_file(*args) -> None:
    """Procesa un archivo de texto con oraciones y guarda los resultados."""
    input_file, output_file = args[:2] if len(args) >= 2 else ('', '')
    pipeline = TextAnalysisPipeline(CORPUS, GRAMMAR_CNF)
    results = []

    # Leer oraciones
    with open(input_file, 'r', encoding='utf-8') as f:
        sentences = f.readlines()

    # Procesar cada oración
    for sentence in sentences:
        sentence = sentence.strip()
        if sentence:
            result = pipeline.process(sentence)
            results.append({
                'sentence': sentence,
                **result
            })

    # Guardar resultados
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

# Ejemplo de uso
# process_file('input_sentences.txt', 'output_results.json')